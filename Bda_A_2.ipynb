{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTBFJlvg2s8yKJpmi9IJWI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laasyakommaraju/r-sutdio/blob/main/Bda_A_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1.Build a classification model with spark with a dataset of your choice in python for big data analysis.\n"
      ],
      "metadata": {
        "id": "PF9IJbRmVxLY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU3iZiDvlziq",
        "outputId": "3951f01f-6cd9-44be-fec8-c6476235c54c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----+----------+\n",
            "|           features|label|prediction|\n",
            "+-------------------+-----+----------+\n",
            "|[35.0,60000.0,25.0]|    0|       1.0|\n",
            "|[28.0,48000.0,18.0]|    0|       0.0|\n",
            "+-------------------+-----+----------+\n",
            "\n",
            "Test AUC: 0.00\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"CustomerPurchasePrediction\").getOrCreate()\n",
        "\n",
        "# Simulated customer data: (age, salary, time_spent_on_website, label)\n",
        "data = [\n",
        "    (25, 45000, 15, 0),  # Customer did not purchase\n",
        "    (30, 55000, 20, 0),\n",
        "    (45, 80000, 40, 1),  # Customer purchased\n",
        "    (35, 60000, 25, 0),\n",
        "    (50, 95000, 35, 1),\n",
        "    (60, 120000, 50, 1),\n",
        "    (40, 70000, 30, 0),\n",
        "    (28, 48000, 18, 0),\n",
        "    (33, 54000, 28, 1),\n",
        "    (38, 75000, 45, 1)\n",
        "]\n",
        "\n",
        "# Define schema\n",
        "columns = [\"age\", \"salary\", \"time_spent_on_website\", \"label\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "# Assemble features\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"age\", \"salary\", \"time_spent_on_website\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "df_prepared = assembler.transform(df).select(\"features\", \"label\")\n",
        "\n",
        "# Split the dataset\n",
        "train_data, test_data = df_prepared.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Decision Tree model\n",
        "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
        "model = dt.fit(train_data)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(test_data)\n",
        "predictions.select(\"features\", \"label\", \"prediction\").show()\n",
        "\n",
        "# Evaluate model\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f\"Test AUC: {auc:.2f}\")\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Build a clustering model with spark with a data set of your choice"
      ],
      "metadata": {
        "id": "_KWRGjvbV6LA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"CustomerSegmentationClustering\").getOrCreate()\n",
        "\n",
        "# New dataset: (annual_income, spending_score)\n",
        "data = [\n",
        "    (15.0, 39.0),\n",
        "    (16.0, 81.0),\n",
        "    (17.0, 6.0),\n",
        "    (18.0, 77.0),\n",
        "    (19.0, 40.0),\n",
        "    (20.0, 76.0),\n",
        "    (21.0, 6.0),\n",
        "    (22.0, 94.0),\n",
        "    (23.0, 3.0),\n",
        "    (24.0, 72.0),\n",
        "    (25.0, 13.0),\n",
        "    (26.0, 70.0),\n",
        "    (27.0, 14.0),\n",
        "    (28.0, 99.0),\n",
        "    (29.0, 15.0)\n",
        "]\n",
        "\n",
        "columns = [\"annual_income\", \"spending_score\"]\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "# Feature assembler\n",
        "assembler = VectorAssembler(inputCols=columns, outputCol=\"features\")\n",
        "df_features = assembler.transform(df).select(\"features\")\n",
        "\n",
        "# KMeans clustering\n",
        "kmeans = KMeans(k=3, seed=1)\n",
        "model = kmeans.fit(df_features)\n",
        "predictions = model.transform(df_features)\n",
        "\n",
        "# Show clustering results\n",
        "predictions.show(truncate=False)\n",
        "\n",
        "# Print cluster centers\n",
        "print(\"Cluster Centers:\")\n",
        "for center in model.clusterCenters():\n",
        "    print(center)\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PX72NYgLmZLe",
        "outputId": "6f5df004-5829-4d83-8851-e7eb111c6157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+\n",
            "|features   |prediction|\n",
            "+-----------+----------+\n",
            "|[15.0,39.0]|2         |\n",
            "|[16.0,81.0]|0         |\n",
            "|[17.0,6.0] |1         |\n",
            "|[18.0,77.0]|0         |\n",
            "|[19.0,40.0]|2         |\n",
            "|[20.0,76.0]|0         |\n",
            "|[21.0,6.0] |1         |\n",
            "|[22.0,94.0]|0         |\n",
            "|[23.0,3.0] |1         |\n",
            "|[24.0,72.0]|0         |\n",
            "|[25.0,13.0]|1         |\n",
            "|[26.0,70.0]|0         |\n",
            "|[27.0,14.0]|1         |\n",
            "|[28.0,99.0]|0         |\n",
            "|[29.0,15.0]|1         |\n",
            "+-----------+----------+\n",
            "\n",
            "Cluster Centers:\n",
            "[22.         81.28571429]\n",
            "[23.66666667  9.5       ]\n",
            "[17.  39.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Build a recommondation engine with spark with a dataset of your choice"
      ],
      "metadata": {
        "id": "ofGVU5sgWWh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"BookRecommendation\").getOrCreate()\n",
        "\n",
        "# New dataset: user_id, book_id, rating\n",
        "data = [\n",
        "    (1, 101, 5.0),\n",
        "    (1, 102, 4.0),\n",
        "    (1, 103, 2.5),\n",
        "    (2, 101, 3.0),\n",
        "    (2, 104, 5.0),\n",
        "    (2, 105, 4.5),\n",
        "    (3, 102, 3.5),\n",
        "    (3, 103, 4.5),\n",
        "    (3, 106, 5.0),\n",
        "    (4, 104, 2.5),\n",
        "    (4, 105, 3.5),\n",
        "    (4, 106, 4.0),\n",
        "    (5, 101, 4.5),\n",
        "    (5, 103, 4.0),\n",
        "    (5, 105, 2.0),\n",
        "]\n",
        "\n",
        "columns = [\"user_id\", \"book_id\", \"rating\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "# Split data into training and test sets\n",
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Build ALS (Alternating Least Squares) model\n",
        "als = ALS(\n",
        "    userCol=\"user_id\",\n",
        "    itemCol=\"book_id\",\n",
        "    ratingCol=\"rating\",\n",
        "    coldStartStrategy=\"drop\",  # Drop rows with NaN predictions\n",
        "    nonnegative=True,  # Ensure that all factors are non-negative\n",
        "    implicitPrefs=False,  # Using explicit ratings\n",
        "    rank=10,  # Number of latent factors\n",
        "    maxIter=10,  # Number of iterations\n",
        "    regParam=0.1  # Regularization parameter to prevent overfitting\n",
        ")\n",
        "\n",
        "# Train the ALS model\n",
        "model = als.fit(train_data)\n",
        "\n",
        "# Predict ratings on test data\n",
        "predictions = model.transform(test_data)\n",
        "predictions.show()\n",
        "\n",
        "# Evaluate the model using RMSE (Root Mean Squared Error)\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Test RMSE: {rmse:.2f}\")\n",
        "\n",
        "# Recommend top 3 books for each user\n",
        "user_recs = model.recommendForAllUsers(3)\n",
        "user_recs.show(truncate=False)\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTgUnlbaqHqr",
        "outputId": "0625ecb3-2672-4849-aef6-9ef647342ba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+------+----------+\n",
            "|user_id|book_id|rating|prediction|\n",
            "+-------+-------+------+----------+\n",
            "|      3|    102|   3.5| 2.0663626|\n",
            "|      5|    101|   4.5|  1.287073|\n",
            "+-------+-------+------+----------+\n",
            "\n",
            "Test RMSE: 2.49\n",
            "+-------+------------------------------------------------------+\n",
            "|user_id|recommendations                                       |\n",
            "+-------+------------------------------------------------------+\n",
            "|1      |[{101, 4.8597326}, {102, 4.0014143}, {106, 3.0055833}]|\n",
            "|2      |[{104, 4.8236704}, {105, 4.45436}, {106, 4.2804103}]  |\n",
            "|3      |[{106, 4.9407706}, {105, 4.1730957}, {104, 3.358101}] |\n",
            "|4      |[{106, 3.9472227}, {105, 3.3039286}, {104, 2.6128924}]|\n",
            "|5      |[{106, 2.1266475}, {105, 1.9698161}, {104, 1.841012}] |\n",
            "+-------+------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}